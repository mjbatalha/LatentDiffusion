flash-attn==2.6.3  # https://github.com/HazyResearch/flash-attention
ninja==1.11.1.1
numpy==2.1.1
torch==2.4.1
transformers==4.45.1
